---
title: "OSTA5003 Week 4 Homework"
subtitle: "Semester 1B, 2023"
format:
    html:
        code-fold: true
        code-line-numbers: true
        number-sections: true
        number-depth: 1
        code-tools: true
        embed-resources: true
---

```{r setup, filename = "Required packages"}
#| message: false
library(tidyverse)
library(caret)
# install.packages("glmnet")
# install.packages("ISLR")
library(glmnet)
library(ISLR)
```

# Major League Baseball

The dataset we are using is one provided by the ISLR package. It contains Major League Baseball Data from the 1986 and 1987 seasons with 322 observations with 20 variables. It has some missing cases that isn't the focus of this module and can be removed with the following code.

```{r}
data(Hitters, package = "ISLR")
Hitters <- na.omit(Hitters)
```

## Stepwise selection

Implement a __backward__ stepwise selection procedure to remove five features. The response feature here is `Salary` which is numeric, so this is a regression and not a classification problem. Use the Root MSE as the performance metric for each selection step. Similar to the lab you may use `caret` to create a training and test split but otherwise implement the solution without using other packages (using `tidyverse` syntax is fine).

```{r}
## Candidate features
features <- colnames(Hitters)
features <- features[features != "Salary"]
### Assess model on all candidates
set.seed(5003)
train_indices <- createDataPartition(Hitters$Salary, p = 0.6)[[1]]
train_data <- Hitters[train_indices, ]
test_data <- Hitters[-train_indices, ]
```

```{r}
# Perform backward stepwise selection
model <- lm(Salary ~ ., data = train_data)
for (i in 1:5) {
  step_model <- step(model, direction = "backward")
  rmse <- sqrt(mean(step_model$residuals^2))
  cat(paste0("Removed feature ", names(coef(model))[which.max(summary(step_model)$coef[, "Pr(>|t|)"])], 
            "; RMSE = ", rmse, "\n"))
  model <- step_model
}
```

## Lasso regression {.tabset .tabset-fade .tabset-pills}

### Split data and fit

Split the dataset into 60% train and 40% test. Perform a lasso regression using the `glmnet` package.

```{r}
library(glmnet)
# Convert data.frame to a design matrix using model.matrix
# Split the dataset into 60% train and 40% test
train_idx <- sample(nrow(Hitters), round(0.6 * nrow(Hitters)))
train_data <- Hitters[train_idx, ]
test_data <- Hitters[-train_idx, ]

# Convert data.frame to a design matrix using model.matrix
train_x <- model.matrix(Salary ~., data = train_data)[,-1]
train_y <- train_data$Salary 

# Fit a LASSO regression using glmnet package
lasso_model <- glmnet(train_x, train_y, alpha = 1) 
```

### Inspect coefficients as function of $\lambda$

Plot the lasso regression coefficients as a function of $\lambda$. Use cross-validation to find the best $\lambda$ value (You can use the inbuilt CV function in glmnet). **Hint** The `cv.glmnet` function will return the best $\lambda$ value as `lambda.min`.

```{r lasso}
# set the range of lambda values to be tested.
grid <- 10^seq(8,-2, length=100)
lasso_model <- glmnet(train_x, train_y, alpha = 1, lambda = grid) 

# Inspect coefficients as a function of lambda
plot(lasso_model, xvar = "lambda", label = TRUE) 

# Use cross-validation to find the best lambda value
cv_model <- cv.glmnet(train_x, train_y, alpha = 1, lambda = grid) 
best_lambda <- cv_model$lambda.min
```

### Check if model is sparse

Inspect the coefficients in the best model above and check if there there are any coefficients that have been shrunk to zero. **Hint** The `cv.glmnet` will also return the model fits for each $\lambda$ value. You can extract the coefficients for each $\lambda$ value using `cv.out$glmnet.fit$beta`.

```{r}
best_lambda <- cv_model$lambda.min

# Check if the model is sparse
coef(lasso_model, s = best_lambda) 

# Inspect the coefficients in the best model 
best_coef <- coef(cv_model, s = best_lambda) 
print(best_coef)
```

### Assess on the test set

Using the optimal trained model predict the salary of the test dataset and calculate the mean squared error.

```{r assessment}
# Use the best CV model to predict the test set Salary

# Assess the model on the test set
test_x <- model.matrix(Salary ~., data = test_data)[, -1]
test_y <- test_data$Salary 

lasso_pred <- predict(lasso_model, newx = test_x, s = best_lambda) 
MSE <- mean((test_y - lasso_pred)^2)
MSE
```
